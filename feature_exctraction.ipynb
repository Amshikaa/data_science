{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1803bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\user\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.2.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.26.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (22.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (58.0.4)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.12.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.59.3)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow-intel==2.15.0->tensorflow)\n",
      "  Using cached keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n",
      "Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.10.0\n",
      "    Uninstalling keras-2.10.0:\n",
      "      Successfully uninstalled keras-2.10.0\n",
      "Successfully installed keras-2.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install tensorflow-text\n",
    "!pip install simpletransformers\n",
    "!pip install spacy\n",
    "!pip install contractions\n",
    "!pip install emoji\n",
    "!pip install emot\n",
    "!pip install demoji\n",
    "!pip install nlpaug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14a5fbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting num2words\n",
      "  Downloading num2words-0.5.13-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting docopt>=0.6.2 (from num2words)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
      "   -------------------------------------- 143.3/143.3 kB 943.7 kB/s eta 0:00:00\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13724 sha256=7cfc4cc589b1d59ec2cbade0a8a7051e1739d04ad639543af698ab022f802955\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\70\\4a\\46\\1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built docopt\n",
      "Installing collected packages: docopt, num2words\n",
      "Successfully installed docopt-0.6.2 num2words-0.5.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\user\\anaconda3\\lib\\site-packages)\n",
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e58be924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_8532/424883077.py:17: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import regex\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "import demoji\n",
    "import emoji\n",
    "from sklearn.metrics import f1_score\n",
    "demoji.download_codes()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from num2words import num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c1b7165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Error loading whitespace: Package 'whitespace' not found\n",
      "[nltk_data]     in index\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('whitespace')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7ab730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Training.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56b8d4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1382343793341575169</td>\n",
       "      <td>@IrvineWelsh I don’t know about you Irvine but...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1377631738692796417</td>\n",
       "      <td>I bet money if i went n took a covid test righ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1386448010029240326</td>\n",
       "      <td>@JamesMelville My wife received a POSITIVE Cov...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1361342676340211717</td>\n",
       "      <td>Out of the 180,000+ people who have had the tw...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1386757983254765569</td>\n",
       "      <td>My whole family is sick af and here I am now i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1382343793341575169  @IrvineWelsh I don’t know about you Irvine but...   \n",
       "1  1377631738692796417  I bet money if i went n took a covid test righ...   \n",
       "2  1386448010029240326  @JamesMelville My wife received a POSITIVE Cov...   \n",
       "3  1361342676340211717  Out of the 180,000+ people who have had the tw...   \n",
       "4  1386757983254765569  My whole family is sick af and here I am now i...   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "871abb78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6266\n",
       "1    1334\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17786cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emot_object = emot.core.emot()\n",
    "ps =PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "english_stopwords = stopwords.words('english')\n",
    "exclude = set(string.punctuation)\n",
    "def preprocess(text):\n",
    "  #text=demoji.findall(df['Text'])\n",
    "  text = contractions.fix(text.lower(), slang=True)\n",
    "  #text= re.sub(r'\\d+', '', text)\n",
    "  text=re.sub(r'$', '', text)\n",
    "  text= re.sub(r'’','', text )\n",
    "  text=re.sub('<.*?>','',text)\n",
    "  text=re.sub(r'http\\S+', '', text)\n",
    "  text = re.sub(r'\\d+', lambda x: num2words(int(x.group(0))), text)\n",
    "\n",
    "  #text=emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "  text = ''.join(ch for ch in text if ch not in exclude)\n",
    "  tokens = word_tokenize(text)\n",
    "\n",
    "  #print(\"Tokens:\", tokens)\n",
    "  text = [t for t in tokens if t not in english_stopwords]\n",
    "  text = \" \".join(text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "082f89fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "#import demoji\n",
    "#demoji.download_codes()\n",
    "def emo(text):\n",
    "  temp=emoji.demojize(text,delimiters=(\" \",\" \"))\n",
    "  temp=temp.replace(\"_\",\"  \")\n",
    "  return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48221dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['emo']=data[\"text\"].apply(lambda x:emo(x))\n",
    "data[\"clean_text\"]=data['emo'].apply(lambda X: preprocess(X))\n",
    "#added_data[\"clean_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4deca68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>emo</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1382343793341575169</td>\n",
       "      <td>@IrvineWelsh I don’t know about you Irvine but...</td>\n",
       "      <td>0</td>\n",
       "      <td>@IrvineWelsh I don’t know about you Irvine but...</td>\n",
       "      <td>irvinewelsh know irvine keep told covid exist ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1377631738692796417</td>\n",
       "      <td>I bet money if i went n took a covid test righ...</td>\n",
       "      <td>0</td>\n",
       "      <td>I bet money if i went n took a covid test righ...</td>\n",
       "      <td>bet money went n took covid test right going t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1386448010029240326</td>\n",
       "      <td>@JamesMelville My wife received a POSITIVE Cov...</td>\n",
       "      <td>0</td>\n",
       "      <td>@JamesMelville My wife received a POSITIVE Cov...</td>\n",
       "      <td>jamesmelville wife received positive covid tes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1361342676340211717</td>\n",
       "      <td>Out of the 180,000+ people who have had the tw...</td>\n",
       "      <td>0</td>\n",
       "      <td>Out of the 180,000+ people who have had the tw...</td>\n",
       "      <td>one hundred eightyzero people two vaccine shot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1386757983254765569</td>\n",
       "      <td>My whole family is sick af and here I am now i...</td>\n",
       "      <td>0</td>\n",
       "      <td>My whole family is sick af and here I am now i...</td>\n",
       "      <td>whole family sick af hospital heart palpitatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1382343793341575169  @IrvineWelsh I don’t know about you Irvine but...   \n",
       "1  1377631738692796417  I bet money if i went n took a covid test righ...   \n",
       "2  1386448010029240326  @JamesMelville My wife received a POSITIVE Cov...   \n",
       "3  1361342676340211717  Out of the 180,000+ people who have had the tw...   \n",
       "4  1386757983254765569  My whole family is sick af and here I am now i...   \n",
       "\n",
       "   label                                                emo  \\\n",
       "0      0  @IrvineWelsh I don’t know about you Irvine but...   \n",
       "1      0  I bet money if i went n took a covid test righ...   \n",
       "2      0  @JamesMelville My wife received a POSITIVE Cov...   \n",
       "3      0  Out of the 180,000+ people who have had the tw...   \n",
       "4      0  My whole family is sick af and here I am now i...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  irvinewelsh know irvine keep told covid exist ...  \n",
       "1  bet money went n took covid test right going t...  \n",
       "2  jamesmelville wife received positive covid tes...  \n",
       "3  one hundred eightyzero people two vaccine shot...  \n",
       "4  whole family sick af hospital heart palpitatio...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f012336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['clean_text'], data['label'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6bb38e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vec = TfidfVectorizer(analyzer='char_wb', ngram_range=(1, 5), max_df=1.0, min_df=1, max_features=5000)\n",
    "count_train = Tfidf_vec.fit(X_train)\n",
    "train_features1 = Tfidf_vec.transform(X_train)\n",
    "test_features1 = Tfidf_vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47946b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vec1 = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), max_df=1.0, min_df=1, max_features=5000)\n",
    "count_train1 = Tfidf_vec1.fit(X_train)\n",
    "train_features2= Tfidf_vec1.transform(X_train)\n",
    "test_features2 = Tfidf_vec1.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "00ecc8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 80.0239\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.82      0.87      2040\n",
      "           1       0.48      0.70      0.57       468\n",
      "\n",
      "    accuracy                           0.80      2508\n",
      "   macro avg       0.70      0.76      0.72      2508\n",
      "weighted avg       0.84      0.80      0.81      2508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf1 =LinearSVC(C=1.0, class_weight=\"balanced\", max_iter=10000, random_state=123)\n",
    "clf1.fit(train_features1, y_train)\n",
    "y_pred1=clf1.predict(test_features1)\n",
    "accuracy = accuracy_score(y_test, y_pred1)\n",
    "\n",
    "print(\"Test Accuracy:\", round(accuracy*100, 4))\n",
    "\n",
    "print(\"\\n\", classification_report(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d6e05cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 81.3397\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.86      0.88      2040\n",
      "           1       0.50      0.62      0.56       468\n",
      "\n",
      "    accuracy                           0.81      2508\n",
      "   macro avg       0.70      0.74      0.72      2508\n",
      "weighted avg       0.83      0.81      0.82      2508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf1 =LinearSVC(C=1.0, class_weight=\"balanced\", max_iter=10000, random_state=123)\n",
    "clf1.fit(train_features2, y_train)\n",
    "y_pred2=clf1.predict(test_features2)\n",
    "accuracy = accuracy_score(y_test, y_pred2)\n",
    "\n",
    "print(\"Test Accuracy:\", round(accuracy*100, 4))\n",
    "\n",
    "print(\"\\n\", classification_report(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcabefc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
