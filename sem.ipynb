{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cc030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Write a python program to\n",
    "#i. read multiple files from single folder\n",
    "#ii. read multiple files from multiple folders\n",
    "#program to read multiple files from single folder\n",
    "import os\n",
    "path = os.getcwd()\n",
    "for file in os.listdir(path):\n",
    "if file.endswith(\".txt\"):\n",
    "file_path = os.path.join(path, file)\n",
    "print(file)\n",
    "with open(file_path, 'r') as f:\n",
    "print(f.read())\n",
    "Output:\n",
    "file1.txt\n",
    "Hello world!!!\n",
    "file2.txt\n",
    "Principles of Data Science\n",
    "file3.txt\n",
    "Welcome\n",
    "#program to read multiple files from multiple folders\n",
    "import os\n",
    "def read_text_files_from_folders(root_folder):\n",
    "for folder_name, subfolders, filenames in os.walk(root_folder):\n",
    "print(\"File name :: \",filenames)\n",
    "for filename in filenames:\n",
    "if filename.endswith('.txt'):\n",
    "file_path = os.path.join(folder_name, filename)\n",
    "try:\n",
    "with open(file_path, 'r') as file:\n",
    "print(folder_name)\n",
    "print(filename)\n",
    "print(file.read())\n",
    "except Exception as e:\n",
    "print(f\"Error reading file {file_path}: {e}\")\n",
    "root_folder = \"F:\\Sample\"\n",
    "texts = read_text_files_from_folders(root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02b51b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Implement the python program to find central tendency (mean, median, and mode)\n",
    "#of data, with and without using built-in function on the data.\n",
    "import numpy as np\n",
    "import statistics\n",
    "def find_mean(list1):\n",
    "total = 0\n",
    "for ele in list1:\n",
    "total += ele\n",
    "mean = total / n\n",
    "return mean\n",
    "def find_median(list1):\n",
    "list1.sort()\n",
    "print(\"Sorted list elements are :: \",list1)\n",
    "if n % 2 == 0:\n",
    "median = (list1[n // 2] + list1[n // 2 - 1]) / 2\n",
    "else:\n",
    "median = list1[n // 2]\n",
    "return median\n",
    "def find_mode(list1):\n",
    "unq_list = []\n",
    "for ele in list1:\n",
    "if ele not in unq_list:\n",
    "unq_list.append(ele)\n",
    "max_count = 0\n",
    "mode_list = []\n",
    "for ele in unq_list:\n",
    "currentCount = list1.count(ele)\n",
    "if currentCount > max_count:\n",
    "max_count = currentCount\n",
    "mode_list = [ele]\n",
    "elif currentCount == max_count:\n",
    "mode_list.append(ele)\n",
    "return mode_list\n",
    "\n",
    "list1 = []\n",
    "n = int(input(\"Enter number of elements :: \"))\n",
    "for i in range(n) :\n",
    "list1.append(int(input(\"Enter a number :: \")))\n",
    "print(\"List elements are :: \",list1)\n",
    "print(\"Mean (built-in) : \", np.mean(list1))\n",
    "print(\"Mean (without built-in) :\",find_mean(list1))\n",
    "print(\"Median (built-in) : \", np.median(list1))\n",
    "print(\"Median (without built-in) :\",find_median(list1))\n",
    "print(\"Mode (built-in) : \", statistics.multimode(list1))\n",
    "print(\"Mode (without built-in) :\",find_mode(list1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2431c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Implement a program to perform measure of dispersion (range, variance, standard\n",
    "#deviation, IQR), with and without using built-in function on the data.\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy import stats\n",
    "def find_range(list1):\n",
    "maxElement = minElement = list1[0]\n",
    "for ele in list1:\n",
    "if ele > maxElement:\n",
    "maxElement = ele\n",
    "if ele < minElement:\n",
    "minElement = ele\n",
    "rangeValue = maxElement - minElement\n",
    "return rangeValue\n",
    "def find_variance(list1):\n",
    "total = 0\n",
    "for ele in list1:\n",
    "total += ele\n",
    "mean = total / len(list1)\n",
    "sumValue = 0\n",
    "for i in list1:\n",
    "sumValue += (i - mean) ** 2\n",
    "variance = sumValue / len(list1)\n",
    "return variance\n",
    "def find_sd(list1):\n",
    "sd = math.sqrt(find_variance(list1))\n",
    "return sd\n",
    "def percentile_midpoint(data, percent):\n",
    "sorted_data = sorted(data)\n",
    "n = len(sorted_data)\n",
    "# Calculate the position of the percentile\n",
    "k = (n - 1) * percent\n",
    "f = int(k) # floor value\n",
    "c = k - f # fractional part\n",
    "if c == 0:\n",
    "# If k is an integer, return the exact value at that position\n",
    "return sorted_data[f]\n",
    "else:\n",
    "# Midpoint interpolation between the two closest ranks\n",
    "return (sorted_data[f] + sorted_data[f + 1]) / 2\n",
    "def find_iqr_midpoint(data):\n",
    "Q1 = percentile_midpoint(data, 0.25) # Calculate the 25th percentile\n",
    "Q3 = percentile_midpoint(data, 0.75) # Calculate the 75th percentile\n",
    "IQR = Q3 - Q1 # Calculate the IQR\n",
    "return IQR\n",
    "list1 = []\n",
    "n=int(input(\"Enter the number of elements ::\"))\n",
    "for i in range(n):\n",
    "ele = int(input(\"Enter the elements ::\"))\n",
    "list1.append(ele)\n",
    "print(\"List elements are\\n\", list1)\n",
    "print(\"The range(without built-in):: \", find_range(list1))\n",
    "maximum = np.max(list1)\n",
    "minimum = np.min(list1)\n",
    "range_value = maximum – minimum\n",
    "print(\"The range(with built-in):: \", range_value)\n",
    "print(\"The variance(without built-in):: \", find_variance(list1))\n",
    "print(\"The variance(with built-in):: \", np.var(list1))\n",
    "print(\"The standard deviation(without built-in):: \", find_sd(list1))\n",
    "print(\"The standard deviation(with built-in):: \", np.std(list1))\n",
    "iqr = find_iqr_midpoint(list1)\n",
    "print(\"Interquartile Range(without built-in function):\", iqr)\n",
    "IQR = stats.iqr(list1, interpolation='midpoint')\n",
    "print(\"IQR(with built-in function):\", IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5379795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. program to perform text data pre-processing with and without using built-in\n",
    "#functions.\n",
    "#With built-in\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "import contractions\n",
    "import re\n",
    "import nltk\n",
    "import emoji\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from num2words import num2words\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('whitespace')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "data = pd.read_csv('Training.tsv',sep='\\t')\n",
    "data.head(10)\n",
    "\n",
    "data['label'].value_counts()\n",
    "\n",
    "ps =PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "english_stopwords = stopwords.words('english')\n",
    "exclude = set(string.punctuation)\n",
    "def preprocess(text):\n",
    "#text=demoji.findall(df['Text'])\n",
    "text = contractions.fix(text.lower(), slang=True)\n",
    "text = re.sub(r'\\d+', lambda x: num2words(int(x.group(0))), text)\n",
    "#text= re.sub(r'\\d+', '', text)\n",
    "text=re.sub(r'$', '', text)\n",
    "text= re.sub(r'’','', text )\n",
    "text=re.sub('<.*?>','',text)\n",
    "text=re.sub(r'http\\S+', '', text)\n",
    "#text=emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "text = ''.join(ch for ch in text if ch not in exclude)\n",
    "tokens = word_tokenize(text)\n",
    "#print(\"Tokens:\", tokens)\n",
    "text = [t for t in tokens if t not in english_stopwords]\n",
    "text = \" \".join(text)\n",
    "return text\n",
    "\n",
    "import emoji\n",
    "#import demoji\n",
    "#demoji.download_codes()\n",
    "def emo(text):\n",
    "temp=emoji.demojize(text,delimiters=(\" \",\" \"))\n",
    "temp=temp.replace(\"_\",\" \")\n",
    "return temp\n",
    "data['emo']=data[\"text\"].apply(lambda x:emo(x))\n",
    "data[\"clean_text\"]=data['emo'].apply(lambda X: preprocess(X))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c7b8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('Training.tsv',sep='\\t')\n",
    "data\n",
    "\n",
    "# Define English stopwords\n",
    "english_stopwords = set(['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\",\n",
    "\"you've\",\"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself','she',\n",
    "\"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'such', 'no', 'nor',\n",
    "'not','only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can','will', 'just', 'don', \"don't\", 'should',\n",
    "\"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've','wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\",\n",
    "'wouldn', \"wouldn't\"])\n",
    "def normalize_apostrophes(text):\n",
    "# Replace different representations of apostrophes with a single consistent representation\n",
    "text = text.replace(\"’\", \"'\") # Replace curly apostrophe with straight apostrophe\n",
    "return text\n",
    "\n",
    "# Define function to preprocess text\n",
    "def preprocess_text(text):\n",
    "# Lowercasing apostrophe\n",
    "text = to_lowercase(text)\n",
    "#Normalising\n",
    "text = normalize_apostrophes(text)\n",
    "# Removing Contractions\n",
    "text = remove_contraction(text)\n",
    "# Converting number to words\n",
    "text = convert_numbers_to_words(text)\n",
    "# Removing URLs\n",
    "text = remove_urls(text)\n",
    "# Removing special characters\n",
    "text = remove_special_characters(text)\n",
    "# Tokenization and removing stopwords\n",
    "tokens = text.split()\n",
    "tokens = [token for token in tokens if token not in english_stopwords]\n",
    "# Joining tokens\n",
    "text = ' '.join(tokens\n",
    "return text\n",
    "def remove_contraction(text):\n",
    "# Define contractions\n",
    "contractions = {\n",
    "\"ain't\": \"am not / is not / are not / has not / have not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is / it has\",\n",
    "\"let's\": \"let us\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is / what has\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "# Expanding contractions\n",
    "for contraction, expansion in contractions.items():\n",
    "text = text.replace(contraction, expansion)\n",
    "return text\n",
    "# Function to remove convert number to words\n",
    "def convert_numbers_to_words(text):\n",
    "# Define a dictionary mapping numeric words to their corresponding words\n",
    "num_words = {\n",
    "'0': 'zero',\n",
    "'1': 'one',\n",
    "'2': 'two',\n",
    "'3': 'three',\n",
    "'4': 'four',\n",
    "'5': 'five',\n",
    "'6': 'six',\n",
    "'7': 'seven',\n",
    "'8': 'eight',\n",
    "'9': 'nine'\n",
    "}\n",
    "# Converting numbers to words\n",
    "for digit, word in num_words.items():\n",
    "text = text.replace(digit, word)\n",
    "return text\n",
    "\n",
    "def to_lowercase(text):\n",
    "lowercase_text = ''\n",
    "for char in text:\n",
    "# Check if character is uppercase\n",
    "if 'A' <= char <= 'Z':\n",
    "# Convert uppercase to lowercase\n",
    "lowercase_text += chr(ord(char) + 32)\n",
    "else:\n",
    "lowercase_text += char\n",
    "return lowercase_text\n",
    "# Function to remove special characters\n",
    "def remove_special_characters(text):\n",
    "# Define special characters\n",
    "special_chars = {'!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=',\n",
    "'>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~'}\n",
    "return ''.join(char for char in text if char not in special_chars)\n",
    "# Function to remove URLs\n",
    "def remove_urls(text):\n",
    "# Split text into words\n",
    "words = text.split()\n",
    "# Filter out words that do not start with 'http' or 'https'\n",
    "filtered_words = [word for word in words if not (word.startswith('http://') or\n",
    "word.startswith('https://'))]\n",
    "# Join the filtered words back into a string\n",
    "return ' '.join(filtered_words)\n",
    "\n",
    "import emoji\n",
    "#import demoji\n",
    "#demoji.download_codes()\n",
    "def emo(text):\n",
    "temp=emoji.demojize(text,delimiters=(\" \",\" \"))\n",
    "temp=temp.replace(\"_\",\" \")\n",
    "return temp\n",
    "data['emo']=data[\"text\"].apply(lambda x:emo(x))\n",
    "data[\"clean_text\"]=data['emo'].apply(lambda X: preprocess_text(X))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30084454",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Write a program to perform Numeric data per-processing with and without using\n",
    "#built-in functions.\n",
    "\n",
    "#With built-in\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"diabetes.csv\")\n",
    "df\n",
    "\n",
    "df.shape\n",
    "\n",
    "df.describe()\n",
    "\n",
    "#seperate features and class label\n",
    "features = df.iloc[:, :-1]\n",
    "class_label = df.iloc[:, -1]\n",
    "def find_duplicates(data):\n",
    "duplicate_rows = data[data.duplicated()]\n",
    "return duplicate_rows\n",
    "duplicate_values = find_duplicates(features)\n",
    "print(\"Duplicates values:\")\n",
    "print(duplicate_values)\n",
    "\n",
    "def remove_duplicates(data):\n",
    "unique_data = data.drop_duplicates()\n",
    "return unique_data\n",
    "features = remove_duplicates(features)\n",
    "print(\"Data after removing duplicates :\")\n",
    "print(features)\n",
    "\n",
    "def find_number_of_missing_values(data):\n",
    "missing_values = data.isnull().sum()\n",
    "# Filter out columns with missing values\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "return missing_values\n",
    "missing_values = find_number_of_missing_values(features)\n",
    "# Print columns with missing values and their respective counts\n",
    "print(\"Columns with missing values:\")\n",
    "print(missing_values)\n",
    "\n",
    "# 1. Handling missing values\n",
    "def handle_missing_values(data, strategy='mean'):\n",
    "if strategy == 'mean':\n",
    "return data.fillna(data.mean())\n",
    "elif strategy == 'max':\n",
    "return data.fillna(data.max())\n",
    "elif strategy == 'min':\n",
    "return data.fillna(data.min())\n",
    "elif strategy == 'zero':\n",
    "return data.fillna(0)\n",
    "elif strategy == 'drop':\n",
    "return data.dropna()\n",
    "features = handle_missing_values(features)\n",
    "print(\"Data after handling missing values:\")\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d9c570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.. Write a python program to read and display various kinds of data (image, text, and\n",
    "#numeric) saved in different format using various python libraries.\n",
    "#Image\n",
    "#code to read and display .png file\n",
    "import cv2\n",
    "image=cv2.imread('flower.png')\n",
    "cv2.imshow(\"image\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "#code to read and display .jpg file\n",
    "import cv2\n",
    "image=cv2.imread('dog.jpg')\n",
    "cv2.imshow(\"image\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "#code to read and display .gif file\n",
    "import cv2\n",
    "def show_gif(file_path):\n",
    "cap = cv2.VideoCapture(file_path)\n",
    "while True:\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "break\n",
    "cv2.imshow('GIF Viewer', frame)\n",
    "if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "show_gif(\"moon.gif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aec93a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Write a python program to read and display video and audio data.\n",
    "#Audio\n",
    "import librosa\n",
    "from IPython.display import Audio\n",
    "# Load audio file\n",
    "audio_path = \"sample-file-4.wav\"\n",
    "y, sr = librosa.load(audio_path)\n",
    "# Play audio\n",
    "Audio(data=y, rate=sr)\n",
    "#Video\n",
    "import cv2\n",
    "# Path to the video file\n",
    "video_path = \"file_example.mp4\"\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "# Check if the video opened successfully\n",
    "if not cap.isOpened():\n",
    "print(\"Error: Could not open the video.\")\n",
    "else:\n",
    "# Create a flag to track window status\n",
    "window_open = True\n",
    "# Loop through each frame in the video\n",
    "while window_open:\n",
    "# Read a frame from the video\n",
    "ret, frame = cap.read()\n",
    "# If the frame was read successfully\n",
    "if ret:\n",
    "# Display the frame\n",
    "cv2.imshow('Video', frame)\n",
    "# Check for the 'q' key to quit\n",
    "if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "break\n",
    "else:\n",
    "# Break the loop if the video has ended\n",
    "break\n",
    "# Check if the window is still open\n",
    "if cv2.getWindowProperty('Video', cv2.WND_PROP_VISIBLE) < 1:\n",
    "window_open = False\n",
    "# Release the video capture object and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58572d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. Write a program to implement a Naive Bayes classifier for sample training dataset.\n",
    "#Also plot the confusion matrix to evaluate the classifier's performance.\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "dataset = pd.read_csv('Social_Network_Ads.csv')\n",
    "dataset\n",
    "\n",
    "X = dataset.iloc[:, [2,3]].values\n",
    "y = dataset.iloc[:,4].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB(priors=[0.4, 0.6], var_smoothing=1e-9)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "#changing hyperparameter values\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Example hyperparameter values\n",
    "custom_priors = [0.3, 0.7] # Custom priors for classes\n",
    "custom_var_smoothing = 1e-8 # Custom var_smoothing value\n",
    "# Initialize Gaussian Naive Bayes classifier with custom hyperparameters\n",
    "classifier = GaussianNB(priors=custom_priors, var_smoothing=custom_var_smoothing)\n",
    "# Assuming X_train and y_train are your training data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "GaussianNB(priors=[0.3, 0.7], var_smoothing=1e-08)\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"The accuracy score is:\", accuracy_score(y_pred, y_test))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"classification_report:\")\n",
    "print( classification_report(y_pred, y_test))\n",
    "\n",
    "plt.imshow(cm,interpolation='nearest',cmap=plt.cm.Blues)\n",
    "plt.title('NaiveBayes - Training Set')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Ture Label')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c0f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. Write a program to implement a Support Vector Machine (SVM) classifier sample\n",
    "#training dataset. Fine-tune various hyperparameters and assess the classifier's\n",
    "#performance on the dataset.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "dataset = pd.read_csv('Social_Network_Ads.csv')\n",
    "Dataset\n",
    "# converting gender column to numeric\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder=LabelEncoder()\n",
    "dataset['Gender']=label_encoder.fit_transform(dataset['Gender'])\n",
    "dataset['Gender'].unique()\n",
    "\n",
    "# to include gender\n",
    "X = dataset.iloc[:, [1, 3]].values\n",
    "\n",
    "y = dataset.iloc[:,4].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "# X = dataset.iloc[:, [2, 3]].values\n",
    "# y = dataset.iloc[:,4].values\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel='poly', random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "Output:\n",
    "SVC(kernel='poly', random_state=0)\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"The accuracy score is:\", accuracy_score(y_pred, y_test))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"classification_report:\")\n",
    "print( classification_report(y_pred, y_test))\n",
    "\n",
    "\n",
    "plt.imshow(cm,interpolation='nearest',cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix(SVM)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.colorbar()\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc87b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10. Write a program to implement Decision Tree classifier. Experiment with different\n",
    "#hyperparameters to evaluate and optimize the classifier's performance.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "dataset = pd.read_csv('Social_Network_Ads.csv')\n",
    "X = dataset.iloc[:, [2, 3]].values\n",
    "y = dataset.iloc[:,4].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "dataset\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion='gini', random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "DecisionTreeClassifier(random_state=0)\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "plt.figure(figsize=(50,50))\n",
    "plot_tree(classifier, feature_names=['Age', 'EstimatedSalary'], class_names=['0', '1'],\n",
    "filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b7d814",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11. Write a program to implement K-means clustering. Using data visualization\n",
    "#(Scatter Plot) technique to illustrate the clustering.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "dataset = pd.read_csv('Mall_Customers.csv')\n",
    "dataset\n",
    "\n",
    "X = dataset.iloc[:, [3, 4]].values\n",
    "# Fitting K-Means to the dataset\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5, init='k-means++', random_state=42)\n",
    "y_kmeans = kmeans.fit_predict(X)\n",
    "# Visualising the clusters\n",
    "plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], c = 'red', label = 'Cluster 1')\n",
    "plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], c = 'blue', label = 'Cluster 2')\n",
    "plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], c = 'green', label = 'Cluster 3')\n",
    "plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], c = 'cyan', label = 'Cluster 4')\n",
    "plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], c = 'magenta', label = 'Cluster 5')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow',\n",
    "label = 'Centroids')\n",
    "\n",
    "\n",
    "plt.title('Clusters of customers')\n",
    "plt.xlabel('Annual Income (k$)')\n",
    "plt.ylabel('Spending Score (1-100)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b866fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12. Write a program to implement hierarchical clustering algorithm. Using data\n",
    "#visualization (Scatter Plot) technique to illustrate the clustering.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"Mall_Customers.csv\")\n",
    "dataset.head()\n",
    "\n",
    "x = dataset.iloc[:,[3,4]].values\n",
    "import scipy.cluster.hierarchy as sch\n",
    "dendrogram = sch.dendrogram(sch.linkage(x, method = 'ward'))\n",
    "plt.title(\"Dendrogram\")\n",
    "plt.xlabel(\"Customer\")\n",
    "plt.ylabel(\"Euclidean distances\")\n",
    "plt.show()\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "hc = AgglomerativeClustering(n_clusters=5, linkage = 'ward')\n",
    "y_hc = hc.fit_predict(x)\n",
    "plt.scatter(x[y_hc == 0, 0], x[y_hc == 0, 1], s = 100, c = \"red\", label = \"cluser 1\")\n",
    "plt.scatter(x[y_hc == 1, 0], x[y_hc == 1, 1], s = 100, c = \"blue\", label = \"cluser 2\")\n",
    "plt.scatter(x[y_hc == 2, 0], x[y_hc == 2, 1], s = 100, c = \"green\", label = \"cluser 3\")\n",
    "plt.scatter(x[y_hc == 3, 0], x[y_hc == 3, 1], s = 100, c = \"cyan\", label = \"cluser 4\")\n",
    "plt.scatter(x[y_hc == 4, 0], x[y_hc == 4, 1], s = 100, c = \"orange\", label = \"cluser 5\")\n",
    "plt.title(\"Clusters of customers\")\n",
    "plt.xlabel(\"Annual Income\")\n",
    "plt.ylabel(\"Spending Score(1-100)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a2c21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#13. Implement density-based clustering using a suitable dataset. Explore the DBSCAN\n",
    "#algorithm and visualize the data.\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "data = pd.read_csv('blobs.csv')\n",
    "data\n",
    "\n",
    "# Extract the features (assuming your CSV file has columns 'Feature1' and 'Feature2')\n",
    "X = data.iloc[:,[0,1]].values\n",
    "X\n",
    "\n",
    "# DBSCAN clustering\n",
    "db = DBSCAN(eps=0.5, min_samples=5)\n",
    "y_db = db.fit_predict(X)\n",
    "# Number of clusters in labels, ignoring noise if present (-1)\n",
    "n_clusters_ = len(set(y_db)) - (1 if -1 in y_db else 0)\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[y_db == 0][:, 0], X[y_db == 0][:, 1], c='blue', marker='o', label='Cluster 1')\n",
    "plt.scatter(X[y_db == 1][:, 0], X[y_db == 1][:, 1], c='green', marker='s', label='Cluster 2')\n",
    "plt.scatter(X[y_db == 2][:, 0], X[y_db == 2][:, 1], c='red', marker='^', label='Cluster 3')\n",
    "plt.scatter(X[y_db == -1][:, 0], X[y_db == -1][:, 1], c='gray', marker='x', label='Noise')\n",
    "plt.legend(loc='best')\n",
    "plt.title(f\"DBSCAN Clustering (Number of Clusters: {n_clusters_})\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc14fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14. Write a program to implement grid-based clustering using a suitable dataset.\n",
    "#Visualize the data using scatter plot.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "# Generate synthetic data\n",
    "data, _ = make_blobs(n_samples=300, centers=4, cluster_std=.60, random_state=0)\n",
    "# Set the grid size (you can adjust this based on your data distribution)\n",
    "grid_size = 1.0\n",
    "# Get the minimum and maximum values for x and y coordinates\n",
    "x_min, x_max = data[:, 0].min(), data[:, 0].max()\n",
    "y_min, y_max = data[:, 1].min(), data[:, 1].max()\n",
    "# Create a grid by defining intervals using the minimum and maximum values\n",
    "x_grid = np.arange(x_min, x_max + grid_size, grid_size)\n",
    "y_grid = np.arange(y_min, y_max + grid_size, grid_size)\n",
    "# Initialize labels array with zeros\n",
    "labels = np.zeros(data.shape[0], dtype=int)\n",
    "# Assign each data point to a grid cell based on its coordinates\n",
    "for i, point in enumerate(data):\n",
    "x, y = point\n",
    "x_label = np.searchsorted(x_grid, x) - 1\n",
    "y_label = np.searchsorted(y_grid, y) - 1\n",
    "labels[i] = x_label * len(y_grid) + y_label\n",
    "# Visualize the clusters\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis')\n",
    "plt.title('Grid-based Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f3d4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#15. Write a program to perform linear regression using\n",
    "#i. Single variable\n",
    "#Ii. Multiple variable\n",
    "#Using single variable\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(\"homeprices.csv\")\n",
    "df\n",
    "\n",
    "plt.xlabel('area')\n",
    "plt.ylabel('price')\n",
    "plt.scatter(df.area, df.price, color = 'blue', marker = '+')\n",
    "\n",
    "\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(df[['area']], df.price)\n",
    "reg.coef_\n",
    "\n",
    "reg.intercept_\n",
    "\n",
    "reg.predict([[6800]])\n",
    "\n",
    "#Using multiple variable\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(\"homeprices_multiple.csv\")\n",
    "df\n",
    "\n",
    "df.bedrooms.median()\n",
    "\n",
    "df.bedrooms = df.bedrooms.fillna(df.bedrooms.median())\n",
    "df\n",
    "\n",
    "x = df.iloc[:, [0,1,2]].values\n",
    "y = df.iloc[:, 3].values\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(x, y)\n",
    "\n",
    "reg.coef_\n",
    "\n",
    "reg.intercept_\n",
    "\n",
    "reg.predict([[2600, 3, 30]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd9f107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#16.chi-square test for feature selection to train SVM\n",
    "#classifier using suitable dataset.\n",
    "import pandas as pd\n",
    "# Load dataset\n",
    "data = pd.read_csv('fruit_data_with_colours.csv')\n",
    "data.head(5)\n",
    "fruit_label = 'fruit_label'\n",
    "fruit_subtype = 'fruit_subtype'\n",
    "fruit_name = 'fruit_name'\n",
    "# Drop non-numeric columns if necessary and extract features (X) and target (y)\n",
    "X = data.drop([fruit_label, fruit_subtype, fruit_name], axis=1) # Features\n",
    "y = data[fruit_label]\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "k_selected_features = 4 # Adjust this value based on how many top features you want to select\n",
    "chi2_selector = SelectKBest(chi2, k=k_selected_features)\n",
    "X_selected = chi2_selector.fit_transform(X, y)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2,random_state=42)\n",
    "# Step 3: Train the SVM classifier\n",
    "from sklearn.svm import SVC\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "SVC(kernel='linear')\n",
    "# Step 4: Evaluate the SVM classifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef447fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#17. Implement a program to perform various data visualization techniques on sample\n",
    "#dataset.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Load dataset (replace with your dataset loading code)\n",
    "data = pd.read_csv('fruit_data_with_colours.csv')\n",
    "# Display the first few rows of the dataset\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(data.head())\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Plot 1: Histogram\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.distplot(data['width'], bins=10, kde=True)\n",
    "plt.title('Histogram of fruit width')\n",
    "# Plot 2: Scatter plot\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.scatterplot(x='width', y='height', data=data)\n",
    "plt.title('Scatter plot of width vs. height')\n",
    "# Plot 3: Box plot\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.boxplot(x='mass', y='color_score', data=data)\n",
    "plt.title('Box plot of mass level vs. color_score')\n",
    "# Plot 4: Count plot\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.countplot(x='fruit_name', data=data)\n",
    "plt.title('Count of fruit_name')\n",
    "# Adjust layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b48826",
   "metadata": {},
   "outputs": [],
   "source": [
    "#18. Implement a program to perform attribute selection measures.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def entropy(data):\n",
    "values, counts = np.unique(data, return_counts=True)\n",
    "probs = counts / len(data)\n",
    "return -np.sum(probs * np.log2(probs))\n",
    "def information_gain(data, attribute_index):\n",
    "total_entropy = entropy(data[:, -1])\n",
    "values, counts = np.unique(data[:, attribute_index], return_counts=True)\n",
    "weighted_entropy = sum((counts[i] / len(data)) * entropy(data[data[:, attribute_index] ==\n",
    "values[i], -1]) for i in range(len(values)))\n",
    "return total_entropy - weighted_entropy\n",
    "def gain_ratio(data, attribute_index):\n",
    "# Information Gain\n",
    "ig = information_gain(data, attribute_index)\n",
    "# Calculate Intrinsic Value\n",
    "values, counts = np.unique(data[:, attribute_index], return_counts=True)\n",
    "total_instances = len(data)\n",
    "intrinsic_value = -np.sum((counts / total_instances) * np.log2(counts / total_instances))\n",
    "return ig / intrinsic_value if intrinsic_value != 0 else 0\n",
    "def gini_index(data):\n",
    "# Calculate the Gini index of a dataset\n",
    "class_labels = data[:, -1]\n",
    "total_instances = len(class_labels)\n",
    "label_counts = np.unique(class_labels, return_counts=True)[1]\n",
    "label_probabilities = label_counts / total_instances\n",
    "gini = 1 - np.sum(label_probabilities**2)\n",
    "return gini\n",
    "def gini_index_attribute(data, attribute_index):\n",
    "# Calculate the Gini index of an attribute in a dataset\n",
    "attribute_values = np.unique(data[:, attribute_index])\n",
    "\n",
    "total_instances = len(data)\n",
    "gini_attribute = 0\n",
    "for value in attribute_values:\n",
    "subset = data[data[:, attribute_index] == value]\n",
    "subset_instances = len(subset)\n",
    "gini_subset = gini_index(subset)\n",
    "gini_attribute += (subset_instances / total_instances) * gini_subset\n",
    "return gini_attribute\n",
    "# Load CSV file\n",
    "df = pd.read_csv(\"Buys_Computer.csv\")\n",
    "data = df.values\n",
    "print(\"Dataset loaded successfully:\")\n",
    "print(df)\n",
    "while True:\n",
    "print(\"\\n1. Information Gain\\n2. Gain Ratio\\n3. Gini Index\\n4. Exit\")\n",
    "ch = input(\"Enter your choice: \")\n",
    "if ch == \"1\":\n",
    "try:\n",
    "attribute_index = int(input(f\"Enter the index of the attribute (0 to {data.shape[1] - 2}) for\n",
    "which you want to calculate Information Gain: \"))\n",
    "if 0 <= attribute_index < data.shape[1] - 1:\n",
    "ig = information_gain(data, attribute_index)\n",
    "print(f\"Information Gain for attribute {attribute_index}: {ig}\")\n",
    "else:\n",
    "print(f\"Invalid attribute index. Please enter a number between 0 and {data.shape[1] -\n",
    "2}.\")\n",
    "except ValueError:\n",
    "print(\"Invalid input. Please enter a valid integer for the attribute index.\")\n",
    "except Exception as e:\n",
    "print(f\"An error occurred: {e}\")\n",
    "elif ch == \"2\":\n",
    "try:\n",
    "                            \n",
    "attribute_index = int(input(f\"Enter the index of the attribute (0 to {data.shape[1] - 2}) for\n",
    "which you want to calculate Gain Ratio: \"))\n",
    "if 0 <= attribute_index < data.shape[1] - 1:\n",
    "# Calculating Gain Ratio for the specified attribute\n",
    "gain_ratio_attr = gain_ratio(data, attribute_index)\n",
    "print(f\"Gain Ratio for attribute {attribute_index}: {gain_ratio_attr}\")\n",
    "else:\n",
    "print(f\"Invalid attribute index. Please enter a number between 0 and {data.shape[1] -\n",
    "2}.\")\n",
    "except ValueError:\n",
    "print(\"Invalid input. Please enter a valid integer for the attribute index.\")\n",
    "except Exception as e:\n",
    "print(f\"An error occurred: {e}\")\n",
    "elif ch == \"3\":\n",
    "try:\n",
    "attribute_index = int(input(f\"Enter the index of the attribute (0 to {data.shape[1] - 2}) for\n",
    "which you want to calculate Gini Index: \"))\n",
    "if 0 <= attribute_index < data.shape[1] - 1:\n",
    "# Calculating Gini index for the specified attribute\n",
    "gini_attr = gini_index_attribute(data, attribute_index)\n",
    "print(f\"Gini index for attribute {attribute_index}: {gini_attr}\")\n",
    "else:\n",
    "print(f\"Invalid attribute index. Please enter a number between 0 and {data.shape[1] -\n",
    "2}.\")\n",
    "except ValueError:\n",
    "print(\"Invalid input. Please enter a valid integer for the attribute index.\")\n",
    "except Exception as e:\n",
    "print(f\"An error occurred: {e}\")\n",
    "elif ch == \"4\":\n",
    "break\n",
    "else:\n",
    "print(\"Invalid choice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7efb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#19. Implement a program to perform different distance measures.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Function to calculate Euclidean distance\n",
    "def euclidean_distance(instance1, instance2):\n",
    "return np.linalg.norm(instance1 - instance2)\n",
    "# Function to calculate Manhattan distance\n",
    "def manhattan_distance(instance1, instance2):\n",
    "return np.sum(np.abs(instance1 - instance2))\n",
    "# Function to calculate Cosine similarity\n",
    "def cosine_similarity(instance1, instance2):\n",
    "dot_product = np.dot(instance1, instance2)\n",
    "norm1 = np.linalg.norm(instance1)\n",
    "norm2 = np.linalg.norm(instance2)\n",
    "return dot_product / (norm1 * norm2)\n",
    "# Load CSV file\n",
    "file_path = 'homeprices_multiple.csv' #input(\"Enter the path to the CSV file: \")\n",
    "df = pd.read_csv(file_path)\n",
    "# Print loaded dataset\n",
    "print(\"Dataset loaded successfully:\")\n",
    "print(df)\n",
    "# Mapping of distance measure names to functions\n",
    "distance_measures = {\n",
    "\"1\": (\"Euclidean\", euclidean_distance),\n",
    "\"2\": (\"Manhattan\", manhattan_distance),\n",
    "\"3\": (\"Cosine Similarity\", cosine_similarity)\n",
    "}\n",
    "# Print distance measure options\n",
    "print(\"\\nSelect a distance measure:\")\n",
    "for key, (measure_name, _) in distance_measures.items():\n",
    "print(f\"{key}. {measure_name}\")\n",
    "# Accept user input for selecting distance measure\n",
    "selected_measure_name = input(\"Enter the index or name of the distance measure: \")\n",
    "# Validate the selected measure\n",
    "if selected_measure_name in distance_measures:\n",
    "selected_measure = distance_measures[selected_measure_name][1] # Get the function\n",
    "corresponding to the selected measure\n",
    "selected_measure_name = distance_measures[selected_measure_name][0] # Get the name\n",
    "of the selected measure\n",
    "else:\n",
    "print(\"Invalid distance measure selection. Please choose from the available options.\")\n",
    "exit()\n",
    "# Input indices of two instances\n",
    "index1 = int(input(f\"Enter index of the first instance (0 to {len(df)-1}): \"))\n",
    "index2 = int(input(f\"Enter index of the second instance (0 to {len(df)-1}): \"))\n",
    "# Validate indices\n",
    "if 0 <= index1 < len(df) and 0 <= index2 < len(df):\n",
    "instance1 = df.iloc[index1, :-1].values # Exclude last column (assuming it's the target variable)\n",
    "instance2 = df.iloc[index2, :-1].values # Exclude last column (assuming it's the target variable)\n",
    "# Calculate distance based on user's choice\n",
    "distance = selected_measure(instance1, instance2)\n",
    "print(f\"{selected_measure_name} distance between instance {index1} and instance {index2}:\n",
    "{distance}\")\n",
    "else:\n",
    "print(f\"Invalid indices. Please enter indices between 0 and {len(df)-1}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f7b298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#20.svc\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['clean_text'], data['label'], test_size=0.33,\n",
    "random_state=42)\n",
    "Feature Extraction: TF-IDF (char_wb)\n",
    "Tfidf_vec1 = TfidfVectorizer(analyzer='char_wb', ngram_range=(1, 5), max_df=1.0,\n",
    "min_df=1, max_features=5000)\n",
    "count_train1 = Tfidf_vec1.fit(X_train)\n",
    "train_features1 = Tfidf_vec1.transform(X_train)\n",
    "test_features1 = Tfidf_vec1.transform(X_test)\n",
    "Feature Extraction- (word) TFIDF\n",
    "Tfidf_vec2 = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), max_df=1.0, min_df=1,\n",
    "max_features=5000)\n",
    "count_train2 = Tfidf_vec2.fit(X_train)\n",
    "train_features2= Tfidf_vec2.transform(X_train)\n",
    "test_features2 = Tfidf_vec2.transform(X_test)\n",
    "Feature Extraction - (word) CountVectorizer\n",
    "count_vec3 = CountVectorizer(analyzer='word', ngram_range=(1, 3), max_df=1.0, min_df=1,\n",
    "max_features=5000)\n",
    "count_train3 = count_vec3.fit(X_train)\n",
    "train_features3 = count_vec3.transform(X_train)\n",
    "test_features3 = count_vec3.transform(X_test)\n",
    "Feature Extraction - (char_wb) CountVectorizer\n",
    "count_vec4 = CountVectorizer(analyzer='char_wb', ngram_range=(1, 5), max_df=1.0, min_df=1,\n",
    "max_features=5000)\n",
    "count_train4 = count_vec4.fit(X_train)\n",
    "train_features4 = count_vec4.transform(X_train)\n",
    "test_features4 = count_vec4.transform(X_test)\n",
    "\n",
    "Model Building with SVM – LinearSVC\n",
    "clf1 =LinearSVC(C=1.0, class_weight=\"balanced\", max_iter=10000, random_state=123)\n",
    "\n",
    "\n",
    "clf1.fit(train_features1, y_train)\n",
    "y_pred1=clf1.predict(test_features1)\n",
    "accuracy = accuracy_score(y_test, y_pred1)\n",
    "print(\"Test Accuracy(Feature Extraction: TF-IDF (char_wb)):\", round(accuracy*100, 4))\n",
    "print(\"\\n\", classification_report(y_test, y_pred1))\n",
    "\n",
    "clf1 =LinearSVC(C=1.0, class_weight=\"balanced\", max_iter=10000, random_state=123)\n",
    "clf1.fit(train_features2, y_train)\n",
    "y_pred2=clf1.predict(test_features2)\n",
    "accuracy = accuracy_score(y_test, y_pred2)\n",
    "print(\"Test Accuracy(Feature Extraction- (word) TFIDF):\", round(accuracy*100, 4))\n",
    "print(\"\\n\", classification_report(y_test, y_pred2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
